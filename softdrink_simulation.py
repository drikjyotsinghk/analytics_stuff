# -*- coding: utf-8 -*-
"""softdrink_simulation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UAMcrfPkEuKWX_IEN093u1enYh6xw8XJ
"""

import pandas as pd
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import RFECV



#simulated softdrink data, with user liking yes/no in the "liked" column
#coca-cola and other companies obviously have survey data
data = {
    "cherry": [random.randint(0, 1) for _ in range(100000)],
    "orange": [random.randint(0, 1) for _ in range(100000)],
    "candy_cane": [random.randint(0, 1) for _ in range(100000)],
    "cotton_candy": [random.randint(0, 1) for _ in range(100000)],
    "grape": [random.randint(0, 1) for _ in range(100000)],
    "liked": [random.randint(0, 1) for _ in range(100000)]
}

df = pd.DataFrame(data)

print(df.head())

#run xgboost to learn which flavours people prefer
x = df[['cherry', 'orange', 'candy_cane', 'cotton_candy', 'grape']]
y = df['liked']

#80-20 split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2,stratify=y)

model = xgb.XGBClassifier()
model.fit(x_train, y_train)

#see what people like the most
feature_importances = model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': x.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

feature_importance_df.head()

sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()

model = xgb.XGBClassifier()
#Fit Recursive feature elimination with cross-validation (RFECV)
rfecv = RFECV(estimator=model, cv=5, scoring='accuracy')
rfecv.fit(x_train, y_train)
print("Optimal number of features: ", rfecv.n_features_)

selected_features = [f for i, f in enumerate(x.columns) if rfecv.support_[i]]

X_train_selected = x_train[selected_features]
X_test_selected = x_test[selected_features]

model.fit(X_train_selected, y_train)
y_pred = model.predict(X_test_selected)

print(selected_features)

